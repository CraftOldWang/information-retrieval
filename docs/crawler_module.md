# 网页抓取模块

## 概述

网页抓取模块负责从南开大学校内网站抓取网页内容，为搜索引擎提供原始数据。根据作业要求，需要抓取至少 10 万个网页。

## 技术选择

### Scrapy 框架

选择使用 Scrapy 作为爬虫框架，理由如下：

1. **高性能**: Scrapy 是一个异步框架，能够高效处理并发请求
2. **功能完备**: 内置了爬虫调度、URL 去重、数据提取等功能
3. **可扩展性**: 支持中间件和管道组件，便于自定义功能
4. **遵循爬虫协议**: 自动遵循 robots.txt 规则，避免不礼貌抓取

## 抓取范围

将针对以下南开大学内网站点进行爬取：

1. 南开大学主站 (https://www.nankai.edu.cn/)
2. 各学院网站
3. 南开新闻网
4. 教务处网站
5. 图书馆网站
6. 校内各部门、研究中心网站
7. 各类校内信息系统(限公开内容)

## 数据抓取策略

### 分布式抓取

采用多个爬虫实例并行工作，通过 Redis 实现 URL 队列和任务调度，加快抓取速度。

### 抓取频率控制

-   每个网站的抓取间隔设置为 1-3 秒
-   避免在校内网络高峰期进行大规模抓取
-   实现 IP 轮换机制，避免单 IP 高频访问

### 数据存储格式

每个网页将被存储为以下格式：

```json
{
    "url": "页面URL",
    "title": "页面标题",
    "content": "页面正文内容",
    "html": "原始HTML",
    "anchor_text": [
        {
            "text": "锚文本内容",
            "href": "链接目标"
        }
    ],
    "attachments": [
        {
            "url": "附件URL",
            "type": "附件类型",
            "filename": "附件文件名",
            "metadata": {
                "title": "文档标题",
                "author": "作者(如果可获取)",
                "upload_date": "上传日期",
                "file_size": "文件大小"
            }
        }
    ],
    "metadata": {
        "crawl_time": "抓取时间",
        "last_modified": "页面最后修改时间",
        "content_type": "内容MIME类型"
    }
}
```

## 文件处理

对于文档类型(PDF、DOC、DOCX 等)，将：

1. 下载并保存原始文档的链接信息
2. 提取文档元数据(标题、作者、上传日期等)，不提取文档内容
3. 将文档元数据索引到 Elasticsearch，不包括文档全文内容

## 爬虫优化

1. **增量爬取**: 定期检查页面更新，只抓取新增或变更内容
2. **智能优先级**: 根据页面重要性和更新频率调整抓取优先级
3. **网页快照**: 存储页面快照，以支持快照查看功能
4. **错误处理**: 实现重试机制和异常处理，确保稳定运行

## 数据预处理

在存入 Elasticsearch 前，将进行以下预处理：

1. 文本清洗(去除 HTML 标签、特殊字符等)
2. 中文分词(使用 jieba 等工具)
3. 提取关键词和摘要
4. 网页去重
5. 对于文档附件，只处理和存储元数据(标题、作者等)，不提取文档全文内容

## 实现计划

1. 设计爬虫架构和数据模型
2. 开发基础爬虫，实现基本抓取功能
3. 完善数据处理和存储功能
4. 优化爬虫性能和稳定性
5. 测试和部署分布式爬虫系统

## 注意事项

1. 严格遵循 robots.txt 规则
2. 仅抓取公开可访问的内容，不尝试绕过任何访问限制
3. 设置合理的抓取频率，避免对校内网络造成负担
4. 妥善保存抓取数据，避免数据丢失
