# NKU æœç´¢å¼•æ“çˆ¬è™«æ¨¡å—å®ç°æ–‡æ¡£

## ğŸ•·ï¸ æ¨¡å—æ¦‚è¿°

æœ¬çˆ¬è™«æ¨¡å—åŸºäº Scrapy æ¡†æ¶å¼€å‘ï¼Œä¸“é—¨ç”¨äºæŠ“å–å—å¼€å¤§å­¦å®˜ç½‘åŠç›¸å…³ç«™ç‚¹çš„ç½‘é¡µå†…å®¹ã€‚æ•´ä¸ªæ¨¡å—é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼ŒåŒ…å«æ•°æ®é¡¹å®šä¹‰ã€çˆ¬è™«é€»è¾‘ã€æ•°æ®å¤„ç†ç®¡é“ã€ä¸­é—´ä»¶å’Œé…ç½®ç®¡ç†ç­‰ç»„ä»¶ã€‚

## ğŸ“ æ–‡ä»¶ç»“æ„

```
crawler/
â”œâ”€â”€ scrapy.cfg              # Scrapyé¡¹ç›®é…ç½®æ–‡ä»¶
â””â”€â”€ nku_spider/             # çˆ¬è™«åŒ…
    â”œâ”€â”€ __init__.py         # åŒ…åˆå§‹åŒ–æ–‡ä»¶
    â”œâ”€â”€ items.py            # æ•°æ®é¡¹å®šä¹‰
    â”œâ”€â”€ middlewares.py      # ä¸­é—´ä»¶
    â”œâ”€â”€ pipelines.py        # æ•°æ®å¤„ç†ç®¡é“
    â”œâ”€â”€ settings.py         # çˆ¬è™«è®¾ç½®
    â””â”€â”€ spiders/            # çˆ¬è™«ç›®å½•
        â”œâ”€â”€ __init__.py     # åŒ…åˆå§‹åŒ–æ–‡ä»¶
        â””â”€â”€ nku_main.py     # ä¸»çˆ¬è™«
```

## ğŸ¯ æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 1. æ•°æ®é¡¹å®šä¹‰ (`items.py`)

å®šä¹‰äº†ä¸‰ç§æ•°æ®ç»“æ„æ¥å­˜å‚¨ä¸åŒç±»å‹çš„çˆ¬å–æ•°æ®ï¼š

#### WebPageItem - ç½‘é¡µæ•°æ®é¡¹

```python
class WebPageItem(scrapy.Item):
    url = scrapy.Field()            # é¡µé¢URL
    title = scrapy.Field()          # é¡µé¢æ ‡é¢˜
    content = scrapy.Field()        # æ­£æ–‡å†…å®¹
    html = scrapy.Field()           # åŸå§‹HTMLä»£ç 
    anchor_texts = scrapy.Field()   # é”šæ–‡æœ¬åˆ—è¡¨
    attachments = scrapy.Field()    # é™„ä»¶ä¿¡æ¯åˆ—è¡¨
    metadata = scrapy.Field()       # å…ƒæ•°æ®ï¼ˆçˆ¬å–æ—¶é—´ã€ä¿®æ”¹æ—¶é—´ç­‰ï¼‰
```

**ä½œç”¨**: æ ‡å‡†åŒ–ç½‘é¡µæ•°æ®æ ¼å¼ï¼Œç¡®ä¿æ•°æ®ç»“æ„ä¸€è‡´æ€§ã€‚

#### DocumentItem - æ–‡æ¡£æ•°æ®é¡¹

```python
class DocumentItem(scrapy.Item):
    url = scrapy.Field()            # æ–‡æ¡£URL
    filename = scrapy.Field()       # æ–‡ä»¶å
    file_type = scrapy.Field()      # æ–‡ä»¶ç±»å‹ï¼ˆpdfã€docç­‰ï¼‰
    file_size = scrapy.Field()      # æ–‡ä»¶å¤§å°
    source_page = scrapy.Field()    # æ¥æºé¡µé¢
    metadata = scrapy.Field()       # æ–‡æ¡£å…ƒæ•°æ®
```

**ä½œç”¨**: å­˜å‚¨ PDFã€DOC ç­‰æ–‡æ¡£çš„å…ƒä¿¡æ¯ï¼Œä¸ä¸‹è½½æ–‡æ¡£å†…å®¹ã€‚

#### AnchorTextItem - é”šæ–‡æœ¬æ•°æ®é¡¹

```python
class AnchorTextItem(scrapy.Item):
    source_url = scrapy.Field()     # æºé¡µé¢URL
    target_url = scrapy.Field()     # ç›®æ ‡é¡µé¢URL
    anchor_text = scrapy.Field()    # é”šæ–‡æœ¬å†…å®¹
    link_context = scrapy.Field()   # é“¾æ¥ä¸Šä¸‹æ–‡
```

**ä½œç”¨**: ä¸“é—¨å­˜å‚¨é¡µé¢é—´çš„é“¾æ¥å…³ç³»ï¼Œç”¨äºåç»­çš„ PageRank è®¡ç®—ã€‚

### 2. ä¸»çˆ¬è™« (`nku_main.py`)

#### çˆ¬è™«é…ç½®

```python
name = "nku_main"
allowed_domains = ["nankai.edu.cn", "www.nankai.edu.cn"]
start_urls = [
    "https://www.nankai.edu.cn/",
    "https://www.nankai.edu.cn/about/",
    # ... æ›´å¤šèµ·å§‹URL
]
```

**è®¾è®¡æ€è·¯**:

-   é™å®šçˆ¬å–åŸŸåï¼Œç¡®ä¿åªæŠ“å–å—å¼€ç›¸å…³å†…å®¹
-   å¤šä¸ªèµ·å§‹ URL æé«˜è¦†ç›–ç‡
-   ä½¿ç”¨ LinkExtractor è‡ªåŠ¨å‘ç°æ–°é“¾æ¥

#### æ ¸å¿ƒæ–¹æ³•è§£æ

**`parse(self, response)` - ä¸»è§£ææ–¹æ³•**

```python
def parse(self, response):
    # 1. æå–é¡µé¢åŸºæœ¬ä¿¡æ¯
    item = WebPageItem()
    item["url"] = response.url
    item["title"] = self.extract_title(response)
    item["content"] = self.extract_content(response)

    # 2. æå–é”šæ–‡æœ¬å’Œé™„ä»¶
    item["anchor_texts"] = self.extract_anchor_texts(response)
    item["attachments"] = self.extract_attachments(response)

    # 3. ç»§ç»­çˆ¬å–å‘ç°çš„é“¾æ¥
    links = self.link_extractor.extract_links(response)
    for link in links:
        yield scrapy.Request(url=link.url, callback=self.parse)

    yield item
```

**`extract_content(self, response)` - å†…å®¹æå–**

```python
def extract_content(self, response):
    # å°è¯•å¤šç§é€‰æ‹©å™¨æå–æ­£æ–‡
    content_selectors = [
        ".main-content", ".content", ".article-content",
        ".news-content", ".page-content", "main", ".container"
    ]

    for selector in content_selectors:
        content_elements = response.css(f"{selector} *::text").getall()
        if content_elements:
            content_text = " ".join([text.strip() for text in content_elements])
            break

    return self.clean_text(content_text)
```

**è®¾è®¡äº®ç‚¹**:

-   **å¤šé€‰æ‹©å™¨ç­–ç•¥**: é€‚åº”ä¸åŒé¡µé¢ç»“æ„
-   **å›é€€æœºåˆ¶**: ç¡®ä¿æ€»èƒ½æå–åˆ°å†…å®¹
-   **æ™ºèƒ½æ¸…æ´—**: å»é™¤æ— ç”¨å­—ç¬¦å’Œç©ºç™½

### 3. æ•°æ®å¤„ç†ç®¡é“ (`pipelines.py`)

é‡‡ç”¨ç®¡é“å¼æ•°æ®å¤„ç†ï¼Œæ¯ä¸ªç®¡é“è´Ÿè´£ç‰¹å®šåŠŸèƒ½ï¼š

#### DataCleaningPipeline - æ•°æ®æ¸…æ´—ç®¡é“

```python
class DataCleaningPipeline:
    def process_item(self, item, spider):
        if "content" in item:
            item["content"] = self.clean_html_content(item["content"])
            item["content"] = self.clean_text(item["content"])
        return item
```

**åŠŸèƒ½**:

-   ç§»é™¤ HTML æ ‡ç­¾å’Œè„šæœ¬
-   æ¸…ç†ç‰¹æ®Šå­—ç¬¦
-   ç»Ÿä¸€ç©ºç™½å­—ç¬¦
-   åˆå§‹åŒ– jieba åˆ†è¯å™¨

#### DuplicatesPipeline - å»é‡ç®¡é“

```python
class DuplicatesPipeline:
    def process_item(self, item, spider):
        url_hash = hashlib.md5(url.encode("utf-8")).hexdigest()

        if self.is_duplicate(url_hash, spider):
            raise DropItem(f"é‡å¤URL: {url}")

        self.mark_processed(url_hash, spider)
        return item
```

**åŠŸèƒ½**:

-   **åˆ†å¸ƒå¼å»é‡**: æ”¯æŒ Redis é›†ç¾¤
-   **æœ¬åœ°å¤‡ä»½**: Redis å¤±è´¥æ—¶ä½¿ç”¨å†…å­˜
-   **å“ˆå¸Œä¼˜åŒ–**: MD5 å‡å°‘å†…å­˜å ç”¨

#### ElasticsearchPipeline - æœç´¢ç´¢å¼•ç®¡é“

```python
class ElasticsearchPipeline:
    def create_indices(self, spider):
        webpage_mapping = {
            "mappings": {
                "properties": {
                    "title": {"type": "text", "analyzer": "ik_max_word"},
                    "content": {"type": "text", "analyzer": "ik_max_word"},
                    "domain": {"type": "keyword"},
                    # ... æ›´å¤šå­—æ®µ
                }
            }
        }
```

**åŠŸèƒ½**:

-   **æ™ºèƒ½ç´¢å¼•**: ä½¿ç”¨ IK ä¸­æ–‡åˆ†è¯å™¨
-   **åµŒå¥—æ–‡æ¡£**: æ”¯æŒé™„ä»¶ç­‰å¤æ‚æ•°æ®
-   **è‡ªåŠ¨ä¼˜åŒ–**: è®¾ç½®åˆ†ç‰‡å’Œå‰¯æœ¬ç­–ç•¥

#### FilePipeline - æ–‡ä»¶å­˜å‚¨ç®¡é“

```python
class FilePipeline:
    def process_item(self, item, spider):
        line = json.dumps(dict(item), ensure_ascii=False) + "\n"
        self.file.write(line)
        return item
```

**åŠŸèƒ½**:

-   **JSONL æ ¼å¼**: ä¾¿äºæ•°æ®åˆ†æ
-   **æŒ‰æ—¥æœŸåˆ†æ–‡ä»¶**: ä¾¿äºç®¡ç†
-   **å¢é‡å†™å…¥**: æ”¯æŒæ–­ç‚¹ç»­ä¼ 

### 4. çˆ¬è™«è®¾ç½® (`settings.py`)

#### æ€§èƒ½é…ç½®

```python
CONCURRENT_REQUESTS = 16          # å¹¶å‘è¯·æ±‚æ•°
DOWNLOAD_DELAY = 2               # è¯·æ±‚é—´éš”
AUTOTHROTTLE_ENABLED = True      # è‡ªåŠ¨é™é€Ÿ
HTTPCACHE_ENABLED = True         # HTTPç¼“å­˜
```

#### ç¤¼è²Œçˆ¬å–

```python
ROBOTSTXT_OBEY = True            # éµå®ˆrobots.txt
USER_AGENT = "Mozilla/5.0..."    # çœŸå®æµè§ˆå™¨æ ‡è¯†
```

#### ç®¡é“é…ç½®

```python
ITEM_PIPELINES = {
    "nku_spider.pipelines.DataCleaningPipeline": 300,
    "nku_spider.pipelines.DuplicatesPipeline": 400,
    "nku_spider.pipelines.ElasticsearchPipeline": 500,
}
```

## ğŸ” ä»£ç é—®é¢˜æ£€æŸ¥

åœ¨æ£€æŸ¥ä»£ç è¿‡ç¨‹ä¸­ï¼Œæˆ‘å‘ç°äº†å‡ ä¸ªéœ€è¦ä¿®æ­£çš„é—®é¢˜ï¼š

### é—®é¢˜ 1: Elasticsearch ç´¢å¼•æ–¹æ³•è¿‡æ—¶

**ä½ç½®**: `pipelines.py` ç¬¬ 264 è¡Œ

```python
# é—®é¢˜ä»£ç 
result = es.index(index=index_name, body=test_doc)

# åº”è¯¥æ”¹ä¸º
result = es.index(index=index_name, document=test_doc)
```

### é—®é¢˜ 2: ç®¡é“é…ç½®ç¼ºå°‘ FilePipeline

**ä½ç½®**: `settings.py` ç®¡é“é…ç½®

```python
# å½“å‰é…ç½®ç¼ºå°‘FilePipeline
ITEM_PIPELINES = {
    "nku_spider.pipelines.DataCleaningPipeline": 300,
    "nku_spider.pipelines.DuplicatesPipeline": 400,
    "nku_spider.pipelines.ElasticsearchPipeline": 500,
    # ç¼ºå°‘ FilePipeline
}
```

### é—®é¢˜ 3: Elasticsearch è¿æ¥é…ç½®éœ€è¦æ›´æ–°

**ä½ç½®**: `pipelines.py` ElasticsearchPipeline

```python
# éœ€è¦æ›´æ–°ä¸ºæ–°ç‰ˆæœ¬çš„è¿æ¥æ–¹å¼
self.es = Elasticsearch(
    ["http://localhost:9200"],
    verify_certs=False,
    ssl_show_warn=False,
    request_timeout=30
)
```

## ğŸ¯ è®¾è®¡è¯„ä¼°

### ä¼˜ç‚¹åˆ†æ

1. **æ¨¡å—åŒ–è®¾è®¡**: å„ç»„ä»¶èŒè´£æ¸…æ™°ï¼Œæ˜“äºç»´æŠ¤
2. **å®¹é”™æœºåˆ¶**: å¤šç§å¤‡é€‰æ–¹æ¡ˆç¡®ä¿ç¨³å®šæ€§
3. **æ€§èƒ½ä¼˜åŒ–**: å¹¶å‘ã€ç¼“å­˜ã€é™é€Ÿç­‰æœºåˆ¶
4. **æ•°æ®è´¨é‡**: å¤šå±‚æ¸…æ´—å’Œå»é‡ä¿è¯

### å¯èƒ½çš„å¤æ‚åŒ–é—®é¢˜

1. **è¿‡å¤šçš„æ•°æ®é¡¹ç±»å‹**: `DocumentItem`å’Œ`AnchorTextItem`å¯èƒ½ç”¨ä¸åˆ°
2. **ç®¡é“è¿‡å¤š**: å¯ä»¥è€ƒè™‘åˆå¹¶ä¸€äº›ç®€å•ç®¡é“
3. **é…ç½®è¿‡äºè¯¦ç»†**: æŸäº›é…ç½®å¯ä»¥ä½¿ç”¨é»˜è®¤å€¼

### ç®€åŒ–å»ºè®®

1. **åˆå¹¶æ•°æ®é¡¹**: å°†æ‰€æœ‰æ•°æ®ç»Ÿä¸€åˆ°`WebPageItem`ä¸­
2. **ç®€åŒ–ç®¡é“**: åˆå¹¶æ¸…æ´—å’Œå»é‡ç®¡é“
3. **å‡å°‘é…ç½®**: å»é™¤ä¸å¿…è¦çš„è‡ªå®šä¹‰é…ç½®

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. å¯åŠ¨çˆ¬è™«

```bash
cd crawler
scrapy crawl nku_main
```

### 2. é™åˆ¶çˆ¬å–æ•°é‡ï¼ˆæµ‹è¯•ç”¨ï¼‰

```bash
scrapy crawl nku_main -s CLOSESPIDER_ITEMCOUNT=100
```

### 3. æŸ¥çœ‹çˆ¬å–ç»Ÿè®¡

```bash
scrapy crawl nku_main -s LOG_LEVEL=INFO
```

## ğŸ“Š é¢„æœŸæ•ˆæœ

-   **çˆ¬å–é€Ÿåº¦**: æ¯å°æ—¶çº¦ 1000-2000 ä¸ªé¡µé¢
-   **æ•°æ®è´¨é‡**: è‡ªåŠ¨å»é‡ï¼Œæ–‡æœ¬æ¸…æ´—
-   **å­˜å‚¨æ ¼å¼**: Elasticsearch ç´¢å¼• + JSONL å¤‡ä»½
-   **å†…å­˜å ç”¨**: åˆ†å¸ƒå¼å»é‡å‡å°‘å†…å­˜å‹åŠ›

## ğŸ”§ åç»­ä¼˜åŒ–æ–¹å‘

1. **å¢é‡çˆ¬å–**: å®šæœŸæ›´æ–°å·²çˆ¬å–é¡µé¢
2. **æ™ºèƒ½è°ƒåº¦**: æ ¹æ®é¡µé¢é‡è¦æ€§è°ƒæ•´ä¼˜å…ˆçº§
3. **å†…å®¹åˆ†æ**: æ·»åŠ å…³é”®è¯æå–å’Œæ‘˜è¦ç”Ÿæˆ
4. **ç›‘æ§ç³»ç»Ÿ**: æ·»åŠ çˆ¬å–çŠ¶æ€ç›‘æ§å’ŒæŠ¥è­¦

è¿™ä¸ªçˆ¬è™«è®¾è®¡åœ¨åŠŸèƒ½å®Œæ•´æ€§å’Œå®ç°å¤æ‚åº¦ä¹‹é—´å–å¾—äº†è¾ƒå¥½çš„å¹³è¡¡ï¼Œæ—¢èƒ½æ»¡è¶³ 10 ä¸‡+é¡µé¢çš„çˆ¬å–éœ€æ±‚ï¼Œåˆä¿æŒäº†ä»£ç çš„å¯ç»´æŠ¤æ€§ã€‚
