#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œå…¨é‡ç½®çˆ¬è™«ç¯å¢ƒ
åˆ é™¤Elasticsearchç´¢å¼•ã€æ¸…ç©ºRedisæ•°æ®ã€å¤‡ä»½JSONLæ–‡ä»¶
"""

import os
import shutil
import json
from datetime import datetime
from elasticsearch import Elasticsearch
import redis
import sys


def reset_elasticsearch():
    """é‡ç½®Elasticsearchï¼Œåˆ é™¤ç´¢å¼•"""
    print("ğŸ”„ é‡ç½®Elasticsearch...")
    print("-" * 50)

    try:
        # è¿æ¥Elasticsearch
        es = Elasticsearch(
            ["http://localhost:9200"],
            verify_certs=False,
            ssl_show_warn=False,
            request_timeout=30,
        )

        if not es.ping():
            print("âŒ Elasticsearchè¿æ¥å¤±è´¥")
            return False

        print("âœ… Elasticsearchè¿æ¥æˆåŠŸ")

        # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        index_name = "nku_webpages"
        if es.indices.exists(index=index_name):
            # åˆ é™¤ç´¢å¼•
            es.indices.delete(index=index_name)
            print(f"ğŸ—‘ï¸ ç´¢å¼• '{index_name}' å·²åˆ é™¤")
        else:
            print(f"â„¹ï¸ ç´¢å¼• '{index_name}' ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤")

        return True
    except Exception as e:
        print(f"âŒ Elasticsearché‡ç½®å¤±è´¥: {e}")
        return False


def reset_redis():
    """é‡ç½®Redisï¼Œæ¸…ç©ºçˆ¬è™«ä½¿ç”¨çš„æ•°æ®"""
    print("\nğŸ”„ é‡ç½®Redis...")
    print("-" * 50)

    try:
        # è¿æ¥Redis
        r = redis.Redis(
            host="localhost",
            port=6379,
            db=0,
            decode_responses=True,
            socket_connect_timeout=5,
        )

        if not r.ping():
            print("âŒ Redisè¿æ¥å¤±è´¥")
            return False

        print("âœ… Redisè¿æ¥æˆåŠŸ")

        # æŸ¥æ‰¾å¹¶åˆ é™¤çˆ¬è™«ç›¸å…³çš„é”®
        # ä¸»è¦æ˜¯ 'crawled_urls' é›†åˆ
        keys = ["crawled_urls"]

        for key in keys:
            if r.exists(key):
                r.delete(key)
                print(f"ğŸ—‘ï¸ Redisé”® '{key}' å·²åˆ é™¤")
            else:
                print(f"â„¹ï¸ Redisé”® '{key}' ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤")

        # æˆ–è€…å½»åº•æ¸…ç©ºå½“å‰æ•°æ®åº“ï¼ˆæ…ç”¨ï¼‰
        # r.flushdb()
        # print("ğŸ—‘ï¸ Redisæ•°æ®åº“å·²æ¸…ç©º")

        return True
    except Exception as e:
        print(f"âŒ Redisé‡ç½®å¤±è´¥: {e}")
        return False


def backup_jsonl_files():
    """å¤‡ä»½ç°æœ‰çš„JSONLæ–‡ä»¶"""
    print("\nğŸ”„ å¤‡ä»½JSONLæ–‡ä»¶...")
    print("-" * 50)

    try:
        # æ•°æ®ç›®å½•
        data_dir = "data/raw"
        backup_dir = "data/backups"

        # åˆ›å»ºå¤‡ä»½ç›®å½•
        os.makedirs(backup_dir, exist_ok=True)

        # å½“å‰æ—¶é—´æˆ³ï¼Œç”¨äºå¤‡ä»½æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # æŸ¥æ‰¾æ‰€æœ‰JSONLæ–‡ä»¶
        jsonl_files = [f for f in os.listdir(data_dir) if f.endswith(".jsonl")]

        if not jsonl_files:
            print("â„¹ï¸ æ²¡æœ‰æ‰¾åˆ°JSONLæ–‡ä»¶ï¼Œæ— éœ€å¤‡ä»½")
            return True

        for file_name in jsonl_files:
            src_path = os.path.join(data_dir, file_name)
            # æ·»åŠ æ—¶é—´æˆ³åˆ°å¤‡ä»½æ–‡ä»¶å
            backup_name = f"{os.path.splitext(file_name)[0]}_{timestamp}.jsonl"
            dst_path = os.path.join(backup_dir, backup_name)

            # å¤åˆ¶æ–‡ä»¶
            shutil.copy2(src_path, dst_path)
            print(f"ğŸ“‘ å·²å¤‡ä»½: {src_path} â†’ {dst_path}")

            # å¯é€‰ï¼šåˆ é™¤åŸæ–‡ä»¶
            os.remove(src_path)
            print(f"ğŸ—‘ï¸ åˆ é™¤åŸå§‹æ–‡ä»¶: {src_path}")

        return True
    except Exception as e:
        print(f"âŒ JSONLæ–‡ä»¶å¤‡ä»½å¤±è´¥: {e}")
        return False


def create_elasticsearch_index():
    """åˆ›å»ºæ–°çš„Elasticsearchç´¢å¼•ï¼Œä½¿ç”¨æ­£ç¡®çš„æ˜ å°„"""
    print("\nğŸ”„ åˆ›å»ºæ–°çš„Elasticsearchç´¢å¼•...")
    print("-" * 50)

    try:
        # è¿æ¥Elasticsearch
        es = Elasticsearch(
            ["http://localhost:9200"],
            verify_certs=False,
            ssl_show_warn=False,
            request_timeout=30,
        )

        if not es.ping():
            print("âŒ Elasticsearchè¿æ¥å¤±è´¥")
            return False

        # ç½‘é¡µç´¢å¼•é…ç½®
        webpage_mapping = {
            "mappings": {
                "properties": {
                    "url": {"type": "keyword"},
                    "title": {
                        "type": "text",
                        "analyzer": "default",
                        "fields": {"keyword": {"type": "keyword"}},
                    },
                    "content": {"type": "text", "analyzer": "default"},
                    "html": {"type": "text", "store": True},  # å­˜å‚¨HTML
                    "anchor_texts": {
                        "type": "nested",  # åµŒå¥—ç±»å‹
                        "properties": {
                            "text": {"type": "text", "analyzer": "default"},
                            "href": {"type": "keyword"},
                        },
                    },
                    "domain": {"type": "keyword"},  # keywordç±»å‹
                    "metadata": {
                        "properties": {
                            "crawl_time": {"type": "date"},
                            "last_modified": {"type": "date"},
                            "content_type": {"type": "keyword"},
                        }
                    },
                    "attachments": {
                        "type": "nested",
                        "properties": {
                            "url": {"type": "keyword"},
                            "filename": {
                                "type": "text",
                                "fields": {"keyword": {"type": "keyword"}},
                            },
                            "type": {"type": "keyword"},
                            "metadata": {
                                "properties": {
                                    "title": {"type": "text", "analyzer": "default"},
                                    "author": {"type": "text"},
                                    "upload_date": {"type": "date"},
                                    "file_size": {"type": "long"},
                                }
                            },
                        },
                    },
                }
            },
            "settings": {
                "number_of_shards": 3,
                "number_of_replicas": 1,
                "analysis": {
                    "analyzer": {"default": {"type": "custom", "tokenizer": "ik_smart"}}
                },
            },
        }

        # åˆ›å»ºç´¢å¼•
        index_name = "nku_webpages"
        es.indices.create(index=index_name, body=webpage_mapping)
        print(f"âœ… ç´¢å¼• '{index_name}' å·²åˆ›å»ºï¼Œä½¿ç”¨æ­£ç¡®çš„æ˜ å°„")

        return True
    except Exception as e:
        print(f"âŒ åˆ›å»ºElasticsearchç´¢å¼•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å®Œå…¨é‡ç½®çˆ¬è™«ç¯å¢ƒ")
    print("=" * 50)

    # ç¡®è®¤æ“ä½œ
    confirm = input("âš ï¸ æ­¤æ“ä½œå°†åˆ é™¤æ‰€æœ‰çˆ¬è™«æ•°æ®ã€‚ç¡®å®šè¦ç»§ç»­å—ï¼Ÿ(y/n): ")
    if confirm.lower() != "y":
        print("âŒ æ“ä½œå·²å–æ¶ˆ")
        return 1

    # é‡ç½®Elasticsearch
    es_result = reset_elasticsearch()

    # é‡ç½®Redis
    redis_result = reset_redis()

    # å¤‡ä»½JSONLæ–‡ä»¶
    jsonl_result = backup_jsonl_files()

    # åˆ›å»ºæ–°çš„ESç´¢å¼•
    index_result = create_elasticsearch_index()

    # æ€»ç»“
    print("\nğŸ“‹ é‡ç½®ç»“æœ:")
    print(f"Elasticsearch: {'âœ… æˆåŠŸ' if es_result else 'âŒ å¤±è´¥'}")
    print(f"Redis: {'âœ… æˆåŠŸ' if redis_result else 'âŒ å¤±è´¥'}")
    print(f"JSONLå¤‡ä»½: {'âœ… æˆåŠŸ' if jsonl_result else 'âŒ å¤±è´¥'}")
    print(f"ESç´¢å¼•åˆ›å»º: {'âœ… æˆåŠŸ' if index_result else 'âŒ å¤±è´¥'}")

    if es_result and redis_result and jsonl_result and index_result:
        print("\nâœ… ç¯å¢ƒé‡ç½®å®Œæˆï¼ç³»ç»Ÿå·²å‡†å¤‡å¥½é‡æ–°çˆ¬å–ã€‚")
        print("\nè¿è¡Œä»¥ä¸‹å‘½ä»¤å¼€å§‹çˆ¬å–:")
        print("cd crawler && scrapy crawl nku_main")
        return 0
    else:
        print("\nâš ï¸ ç¯å¢ƒé‡ç½®è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·æŸ¥çœ‹ä¸Šé¢çš„æ—¥å¿—ã€‚")
        return 1


if __name__ == "__main__":
    sys.exit(main())
