#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ£€æŸ¥Elasticsearchä¸­çš„å®é™…æ–‡æ¡£æ•°é‡
æ’é™¤åµŒå¥—å¯¹è±¡å¯¼è‡´çš„è®¡æ•°é—®é¢˜
"""

import json
from elasticsearch import Elasticsearch


def check_document_counts():
    """æ£€æŸ¥å®é™…æ–‡æ¡£æ•°é‡ä¸åµŒå¥—å¯¹è±¡æ•°é‡"""
    print("ğŸ”¢ æ£€æŸ¥Elasticsearchä¸­çš„å®é™…æ–‡æ¡£æ•°é‡")
    print("=" * 50)

    # è¿æ¥Elasticsearch
    es = Elasticsearch(
        ["http://localhost:9200"],
        verify_certs=False,
        ssl_show_warn=False,
        request_timeout=30,
    )

    if not es.ping():
        print("âŒ Elasticsearchè¿æ¥å¤±è´¥")
        return False

    print("âœ… Elasticsearchè¿æ¥æˆåŠŸ")

    # è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
    try:
        stats = es.indices.stats(index="nku_webpages")
        total_doc_count = stats["indices"]["nku_webpages"]["total"]["docs"]["count"]
        print(f"ğŸ“Š ESæŠ¥å‘Šçš„æ€»æ–‡æ¡£æ•°: {total_doc_count}")
    except Exception as e:
        print(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        return False

    # è·å–é¡¶çº§æ–‡æ¡£æ•°é‡
    try:
        # ä½¿ç”¨cardinalityèšåˆæ¥è·å–å”¯ä¸€URLæ•°é‡
        response = es.search(
            index="nku_webpages",
            body={
                "size": 0,
                "aggs": {"unique_urls": {"cardinality": {"field": "url"}}},
            },
        )

        # è·å–å®é™…ç½‘é¡µæ–‡æ¡£æ•°é‡
        unique_urls = response["aggregations"]["unique_urls"]["value"]
        print(f"ğŸ“„ å®é™…ç½‘é¡µæ–‡æ¡£æ•°(URLå”¯ä¸€è®¡æ•°): {unique_urls}")

        # è·å–ç½‘é¡µåˆ—è¡¨
        print(f"\nğŸ“‘ è·å–å®é™…ç½‘é¡µåˆ—è¡¨...")
        response = es.search(
            index="nku_webpages",
            body={
                "query": {"match_all": {}},
                "_source": ["url", "title", "domain"],
                "size": unique_urls,
            },
        )

        hits = response["hits"]["hits"]
        print(f"âœ… æ‰¾åˆ° {len(hits)} ä¸ªç½‘é¡µæ–‡æ¡£")

        # æ˜¾ç¤ºæ¯ä¸ªç½‘é¡µä¿¡æ¯
        print("\nğŸ“‹ ç½‘é¡µæ–‡æ¡£åˆ—è¡¨:")
        for i, hit in enumerate(hits, 1):
            source = hit["_source"]
            print(
                f"{i}. {source.get('title', 'N/A')[:50]}... [{source.get('domain', 'N/A')}]"
            )

        # åˆ†æåµŒå¥—å¯¹è±¡æ•°é‡
        print("\nğŸ” åˆ†æåµŒå¥—å¯¹è±¡æ•°é‡...")

        # æ£€æŸ¥anchor_textsåµŒå¥—å­—æ®µ
        nested_query = {
            "size": 0,
            "aggs": {"nested_count": {"nested": {"path": "anchor_texts"}}},
        }

        response = es.search(index="nku_webpages", body=nested_query)

        anchor_texts_count = response["aggregations"]["nested_count"]["doc_count"]
        print(f"ğŸ”— é”šæ–‡æœ¬(anchor_texts)åµŒå¥—å¯¹è±¡æ•°é‡: {anchor_texts_count}")

        # æ£€æŸ¥attachmentsåµŒå¥—å­—æ®µ
        nested_query["aggs"]["nested_count"]["nested"]["path"] = "attachments"
        response = es.search(index="nku_webpages", body=nested_query)

        attachments_count = response["aggregations"]["nested_count"]["doc_count"]
        print(f"ğŸ“ é™„ä»¶(attachments)åµŒå¥—å¯¹è±¡æ•°é‡: {attachments_count}")

        # è®¡ç®—æ€»æ•°
        estimated_total = unique_urls + anchor_texts_count + attachments_count
        print(f"\nğŸ“Š æ–‡æ¡£æ•°é‡åˆ†æ:")
        print(f"  - ç½‘é¡µæ–‡æ¡£æ•°: {unique_urls}")
        print(f"  - é”šæ–‡æœ¬åµŒå¥—å¯¹è±¡æ•°: {anchor_texts_count}")
        print(f"  - é™„ä»¶åµŒå¥—å¯¹è±¡æ•°: {attachments_count}")
        print(f"  - ä¼°è®¡æ€»æ•°: {estimated_total}")
        print(f"  - ESæŠ¥å‘Šæ€»æ•°: {total_doc_count}")
        print(f"  - å·®å¼‚: {total_doc_count - estimated_total}")

    except Exception as e:
        print(f"âŒ åˆ†ææ–‡æ¡£æ•°é‡å¤±è´¥: {e}")
        return False

    return True


if __name__ == "__main__":
    check_document_counts()
