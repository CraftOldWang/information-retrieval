#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸´æ—¶ä¿®å¤Elasticsearchç´¢å¼•æ˜ å°„é…ç½®
åœ¨IKåˆ†è¯å™¨æ’ä»¶å®‰è£…ä¹‹å‰ä½¿ç”¨æ ‡å‡†åˆ†æå™¨
"""

import sys
from pathlib import Path
import json


def create_temp_mapping():
    """åˆ›å»ºä¸ä¾èµ–IKåˆ†è¯å™¨çš„ä¸´æ—¶æ˜ å°„é…ç½®"""
    temp_mapping = {
        "mappings": {
            "properties": {
                "url": {"type": "keyword"},
                "title": {
                    "type": "text",
                    "analyzer": "standard",  # ä½¿ç”¨æ ‡å‡†åˆ†æå™¨æ›¿ä»£ik_max_word
                    "fields": {"keyword": {"type": "keyword"}},
                },
                "content": {"type": "text", "analyzer": "standard"},  # ä½¿ç”¨æ ‡å‡†åˆ†æå™¨
                "anchor_texts": {
                    "type": "text",
                    "analyzer": "standard",
                },  # ä½¿ç”¨æ ‡å‡†åˆ†æå™¨
                "domain": {"type": "keyword"},
                "crawl_time": {"type": "date"},
                "last_modified": {"type": "date"},
                "content_type": {"type": "keyword"},
                "file_size": {"type": "integer"},
                "links_count": {"type": "integer"},
                "depth": {"type": "integer"},
            }
        },
        "settings": {
            "number_of_shards": 1,
            "number_of_replicas": 0,
            # æš‚æ—¶ä¸ä½¿ç”¨IKåˆ†è¯å™¨ç›¸å…³è®¾ç½®
            "analysis": {"analyzer": {"default": {"type": "standard"}}},
        },
    }
    return temp_mapping


def update_pipelines_temp():
    """ä¸´æ—¶æ›´æ–°pipelines.pyï¼Œç§»é™¤IKåˆ†è¯å™¨ä¾èµ–"""
    pipelines_file = Path("../crawler/nku_spider/pipelines.py")

    if not pipelines_file.exists():
        print("âŒ pipelines.py æ–‡ä»¶ä¸å­˜åœ¨")
        return False

    try:
        # è¯»å–ç°æœ‰æ–‡ä»¶
        with open(pipelines_file, "r", encoding="utf-8") as f:
            content = f.read()

        # æ›¿æ¢IKåˆ†è¯å™¨ç›¸å…³é…ç½®
        content = content.replace('"analyzer": "ik_max_word"', '"analyzer": "standard"')
        content = content.replace(
            '"analyzer": "ik_smart_pinyin"', '"analyzer": "standard"'
        )
        content = content.replace(
            '"search_analyzer": "ik_smart"', '"search_analyzer": "standard"'
        )

        # å¤‡ä»½åŸæ–‡ä»¶
        backup_file = pipelines_file.with_suffix(".py.backup")
        with open(backup_file, "w", encoding="utf-8") as f:
            f.write(content)

        # å†™å…¥ä¸´æ—¶ä¿®å¤ç‰ˆæœ¬
        with open(pipelines_file, "w", encoding="utf-8") as f:
            f.write(content)

        print("âœ… pipelines.py å·²ä¸´æ—¶ä¿®å¤ï¼Œç§»é™¤äº†IKåˆ†è¯å™¨ä¾èµ–")
        print(f"ğŸ“ åŸæ–‡ä»¶å¤‡ä»½ä¸º: {backup_file}")
        return True

    except Exception as e:
        print(f"âŒ æ›´æ–°pipelines.pyå¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ ä¸´æ—¶ä¿®å¤Elasticsearché…ç½®...")
    print("=" * 40)

    # åˆ›å»ºä¸´æ—¶æ˜ å°„é…ç½®
    temp_mapping = create_temp_mapping()

    # ä¿å­˜æ˜ å°„é…ç½®åˆ°æ–‡ä»¶
    config_dir = Path("../config/elasticsearch")
    config_dir.mkdir(parents=True, exist_ok=True)

    mapping_file = config_dir / "temp_mapping.json"
    with open(mapping_file, "w", encoding="utf-8") as f:
        json.dump(temp_mapping, f, indent=2, ensure_ascii=False)

    print(f"âœ… ä¸´æ—¶æ˜ å°„é…ç½®å·²ä¿å­˜: {mapping_file}")

    # æ›´æ–°pipelines.py
    if update_pipelines_temp():
        print("\nğŸ‰ ä¸´æ—¶ä¿®å¤å®Œæˆï¼")
        print("ğŸ“ è¯´æ˜:")
        print("  - å·²ç§»é™¤å¯¹IKåˆ†è¯å™¨çš„ä¾èµ–")
        print("  - ä½¿ç”¨æ ‡å‡†åˆ†æå™¨æ›¿ä»£")
        print("  - ä¸­æ–‡åˆ†è¯æ•ˆæœå¯èƒ½ä¸å¦‚IKï¼Œä½†ä¸ä¼šæŠ¥é”™")
        print("  - å®‰è£…IKæ’ä»¶åè¯·æ¢å¤åŸé…ç½®")
        print("\nğŸš€ ç°åœ¨å¯ä»¥é‡æ–°è¿è¡Œçˆ¬è™«æµ‹è¯•:")
        print("  cd crawler && scrapy crawl nku_main")
        return True
    else:
        print("\nâŒ ä¸´æ—¶ä¿®å¤å¤±è´¥")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
