#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ£€æŸ¥ElasticsearchçŠ¶æ€å’Œæ•°æ®çš„è„šæœ¬
ç”¨äºéªŒè¯çˆ¬å–çš„æ•°æ®æ˜¯å¦æ­£ç¡®å­˜å‚¨åœ¨ESä¸­
"""

import json
import sys
from datetime import datetime
from elasticsearch import Elasticsearch
from elasticsearch.exceptions import ConnectionError, NotFoundError


def check_es_connection():
    """æ£€æŸ¥Elasticsearchè¿æ¥"""
    try:
        es = Elasticsearch(
            ["http://localhost:9200"],
            verify_certs=False,
            ssl_show_warn=False,
            request_timeout=30,
        )

        if es.ping():
            print("âœ… Elasticsearchè¿æ¥æˆåŠŸ")

            # è·å–é›†ç¾¤ä¿¡æ¯
            cluster_info = es.info()
            print(f"ğŸ“Š ESç‰ˆæœ¬: {cluster_info['version']['number']}")
            print(f"ğŸ“Š é›†ç¾¤åç§°: {cluster_info['cluster_name']}")
            return es
        else:
            print("âŒ Elasticsearchè¿æ¥å¤±è´¥")
            return None

    except ConnectionError as e:
        print(f"âŒ æ— æ³•è¿æ¥åˆ°Elasticsearch: {e}")
        return None
    except Exception as e:
        print(f"âŒ Elasticsearchè¿æ¥å¼‚å¸¸: {e}")
        return None


def check_indices(es):
    """æ£€æŸ¥ç´¢å¼•çŠ¶æ€"""
    try:  # è·å–æ‰€æœ‰ç´¢å¼•
        indices = es.indices.get_alias(index="*")
        print(f"\nğŸ“ å½“å‰ç´¢å¼•åˆ—è¡¨:")

        target_index = "nku_webpages"
        index_exists = False

        for index_name in indices:
            print(f"  - {index_name}")
            if index_name == target_index:
                index_exists = True

        if index_exists:
            print(f"\nâœ… ç›®æ ‡ç´¢å¼• '{target_index}' å­˜åœ¨")
            # è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
            stats = es.indices.stats(index=target_index)
            doc_count = stats["indices"][target_index]["total"]["docs"]["count"]
            store_size = stats["indices"][target_index]["total"]["store"][
                "size_in_bytes"
            ]

            print(f"ğŸ“Š æ–‡æ¡£æ•°é‡: {doc_count}")
            print(f"ğŸ“Š å­˜å‚¨å¤§å°: {store_size / 1024 / 1024:.2f} MB")

            # è·å–ç´¢å¼•æ˜ å°„
            mapping = es.indices.get_mapping(index=target_index)
            properties = mapping[target_index]["mappings"].get("properties", {})
            print(f"ğŸ“Š å­—æ®µæ•°é‡: {len(properties)}")
            print(f"ğŸ“Š å­—æ®µåˆ—è¡¨: {list(properties.keys())}")

            return target_index, doc_count
        else:
            print(f"\nâŒ ç›®æ ‡ç´¢å¼• '{target_index}' ä¸å­˜åœ¨")
            return None, 0

    except Exception as e:
        print(f"âŒ æ£€æŸ¥ç´¢å¼•æ—¶å‡ºé”™: {e}")
        return None, 0


def check_data_samples(es, index_name, doc_count):
    """æ£€æŸ¥æ•°æ®æ ·æœ¬"""
    if not index_name or doc_count == 0:
        print("\nâš ï¸ æ²¡æœ‰æ•°æ®å¯ä»¥æ£€æŸ¥")
        return

    try:
        print(f"\nğŸ” æ£€æŸ¥æ•°æ®æ ·æœ¬...")
        # è·å–å‰5ä¸ªæ–‡æ¡£
        response = es.search(
            index=index_name,
            query={"match_all": {}},
            size=5,
            source=["url", "title", "domain", "crawl_time"],
        )

        hits = response["hits"]["hits"]
        print(f"ğŸ“„ å‰{len(hits)}ä¸ªæ–‡æ¡£æ ·æœ¬:")

        for i, hit in enumerate(hits, 1):
            source = hit["_source"]
            print(f"\n  {i}. æ–‡æ¡£ID: {hit['_id']}")
            print(f"     URL: {source.get('url', 'N/A')}")
            print(f"     æ ‡é¢˜: {source.get('title', 'N/A')[:50]}...")
            print(f"     åŸŸå: {source.get('domain', 'N/A')}")
            print(f"     çˆ¬å–æ—¶é—´: {source.get('crawl_time', 'N/A')}")
        # æ£€æŸ¥åŸŸååˆ†å¸ƒ
        print(f"\nğŸŒ åŸŸååˆ†å¸ƒ:")
        domain_agg = es.search(
            index=index_name,
            query={"match_all": {}},
            size=0,
            aggs={"domains": {"terms": {"field": "domain", "size": 10}}},
        )

        domain_buckets = domain_agg["aggregations"]["domains"]["buckets"]
        for bucket in domain_buckets:
            print(f"  - {bucket['key']}: {bucket['doc_count']} ä¸ªæ–‡æ¡£")

        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        print(f"\nğŸ” æ•°æ®å®Œæ•´æ€§æ£€æŸ¥:")
        # æ£€æŸ¥å¿…å¡«å­—æ®µ
        required_fields = ["url", "title", "content"]
        for field in required_fields:
            missing_query = {"bool": {"must_not": {"exists": {"field": field}}}}

            missing_count = es.count(index=index_name, query=missing_query)["count"]
            if missing_count == 0:
                print(f"  âœ… {field}: æ‰€æœ‰æ–‡æ¡£éƒ½æœ‰æ­¤å­—æ®µ")
            else:
                print(f"  âš ï¸ {field}: {missing_count} ä¸ªæ–‡æ¡£ç¼ºå°‘æ­¤å­—æ®µ")
        # æ£€æŸ¥å†…å®¹é•¿åº¦åˆ†å¸ƒ
        print(f"\nğŸ“ å†…å®¹é•¿åº¦æ£€æŸ¥:")
        content_stats = es.search(
            index=index_name,
            query={"match_all": {}},
            size=0,
            aggs={
                "content_length": {
                    "stats": {
                        "script": {
                            "source": "doc['content.keyword'].size() > 0 ? doc['content.keyword'].value.length() : 0"
                        }
                    }
                }
            },
        )

        if "aggregations" in content_stats:
            stats = content_stats["aggregations"]["content_length"]
            print(f"  ğŸ“Š å¹³å‡é•¿åº¦: {stats['avg']:.0f} å­—ç¬¦")
            print(f"  ğŸ“Š æœ€çŸ­: {stats['min']:.0f} å­—ç¬¦")
            print(f"  ğŸ“Š æœ€é•¿: {stats['max']:.0f} å­—ç¬¦")

    except Exception as e:
        print(f"âŒ æ£€æŸ¥æ•°æ®æ ·æœ¬æ—¶å‡ºé”™: {e}")


def compare_with_jsonl():
    """ä¸JSONLæ–‡ä»¶è¿›è¡Œå¯¹æ¯”"""
    try:
        print(f"\nğŸ“‹ ä¸JSONLæ–‡ä»¶å¯¹æ¯”:")

        jsonl_file = "../data/raw/crawled_data_2025-06-10.jsonl"
        with open(jsonl_file, "r", encoding="utf-8") as f:
            jsonl_count = sum(1 for line in f)

        print(f"  ğŸ“„ JSONLæ–‡ä»¶ä¸­çš„æ¡ç›®æ•°: {jsonl_count}")

        # è¯»å–URLåˆ—è¡¨è¿›è¡Œå¯¹æ¯”
        urls_in_jsonl = []
        with open(jsonl_file, "r", encoding="utf-8") as f:
            for line in f:
                data = json.loads(line.strip())
                urls_in_jsonl.append(data["url"])

        return jsonl_count, urls_in_jsonl

    except FileNotFoundError:
        print(f"  âš ï¸ æœªæ‰¾åˆ°JSONLæ–‡ä»¶")
        return 0, []
    except Exception as e:
        print(f"  âŒ è¯»å–JSONLæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return 0, []


def check_url_consistency(es, index_name, urls_in_jsonl):
    """æ£€æŸ¥URLä¸€è‡´æ€§"""
    if not index_name or not urls_in_jsonl:
        return

    try:
        print(f"\nğŸ”— URLä¸€è‡´æ€§æ£€æŸ¥:")
        # è·å–ESä¸­çš„æ‰€æœ‰URL
        response = es.search(
            index=index_name,
            query={"match_all": {}},
            size=1000,  # å‡è®¾ä¸ä¼šè¶…è¿‡1000ä¸ªæ–‡æ¡£
            source=["url"],
        )

        urls_in_es = [hit["_source"]["url"] for hit in response["hits"]["hits"]]

        print(f"  ğŸ“„ JSONLä¸­çš„URLæ•°é‡: {len(urls_in_jsonl)}")
        print(f"  ğŸ“„ ESä¸­çš„URLæ•°é‡: {len(urls_in_es)}")

        # æ‰¾å‡ºå·®å¼‚
        urls_only_in_jsonl = set(urls_in_jsonl) - set(urls_in_es)
        urls_only_in_es = set(urls_in_es) - set(urls_in_jsonl)

        if urls_only_in_jsonl:
            print(f"  âš ï¸ åªåœ¨JSONLä¸­å­˜åœ¨çš„URL: {len(urls_only_in_jsonl)}")
            for url in list(urls_only_in_jsonl)[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                print(f"    - {url}")

        if urls_only_in_es:
            print(f"  âš ï¸ åªåœ¨ESä¸­å­˜åœ¨çš„URL: {len(urls_only_in_es)}")
            for url in list(urls_only_in_es)[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                print(f"    - {url}")

        if not urls_only_in_jsonl and not urls_only_in_es:
            print(f"  âœ… JSONLå’ŒESä¸­çš„URLå®Œå…¨ä¸€è‡´")

    except Exception as e:
        print(f"âŒ æ£€æŸ¥URLä¸€è‡´æ€§æ—¶å‡ºé”™: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” Elasticsearchæ•°æ®æ£€æŸ¥å·¥å…·")
    print("=" * 50)

    # 1. æ£€æŸ¥è¿æ¥
    es = check_es_connection()
    if not es:
        print("\nâŒ æ— æ³•è¿æ¥åˆ°Elasticsearchï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯åŠ¨")
        return False

    # 2. æ£€æŸ¥ç´¢å¼•
    index_name, doc_count = check_indices(es)

    # 3. æ£€æŸ¥æ•°æ®æ ·æœ¬
    check_data_samples(es, index_name, doc_count)

    # 4. ä¸JSONLæ–‡ä»¶å¯¹æ¯”
    jsonl_count, urls_in_jsonl = compare_with_jsonl()

    # 5. æ£€æŸ¥URLä¸€è‡´æ€§
    check_url_consistency(es, index_name, urls_in_jsonl)

    # 6. æ€»ç»“
    print(f"\nğŸ“Š æ€»ç»“:")
    print(f"  - Elasticsearchä¸­æœ‰ {doc_count} ä¸ªæ–‡æ¡£")
    print(f"  - JSONLæ–‡ä»¶ä¸­æœ‰ {jsonl_count} ä¸ªæ¡ç›®")

    if doc_count > 0 and jsonl_count > 0:
        if doc_count == jsonl_count:
            print(f"  âœ… æ•°æ®é‡ä¸€è‡´ï¼Œçœ‹èµ·æ¥çˆ¬è™«å·¥ä½œæ­£å¸¸")
            print(f"  ğŸš€ å¯ä»¥å¼€å§‹å¤§è§„æ¨¡çˆ¬å–äº†ï¼")
        else:
            print(f"  âš ï¸ æ•°æ®é‡ä¸ä¸€è‡´ï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥çˆ¬è™«é…ç½®")
    elif doc_count == 0:
        print(f"  âŒ ESä¸­æ²¡æœ‰æ•°æ®ï¼Œå¯èƒ½å­˜å‚¨è¿‡ç¨‹æœ‰é—®é¢˜")

    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
