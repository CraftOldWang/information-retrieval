#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤Elasticsearchçš„ç´¢å¼•æ˜ å°„
è§£å†³domainå­—æ®µèšåˆæ“ä½œçš„é—®é¢˜
"""

import json
import os
import hashlib
from urllib.parse import urlparse
from elasticsearch import Elasticsearch


def fix_es_mapping():
    """ä¿®å¤ESç´¢å¼•æ˜ å°„"""
    print("ğŸ”§ ä¿®å¤Elasticsearchç´¢å¼•æ˜ å°„")
    print("=" * 50)

    # åˆå§‹åŒ–docså˜é‡ï¼Œé¿å…å¯èƒ½æœªå®šä¹‰çš„é”™è¯¯
    docs = []

    # è¿æ¥Elasticsearch
    es = Elasticsearch(
        ["http://localhost:9200"],
        verify_certs=False,
        ssl_show_warn=False,
        request_timeout=30,
    )

    if not es.ping():
        print("âŒ Elasticsearchè¿æ¥å¤±è´¥")
        return False

    print("âœ… Elasticsearchè¿æ¥æˆåŠŸ")

    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
    index_name = "nku_webpages"
    if es.indices.exists(index=index_name):
        # å¤‡ä»½æ•°æ®
        print(f"ğŸ“¦ å¤‡ä»½ç°æœ‰æ•°æ®...")
        try:
            response = es.search(
                index=index_name,
                query={"match_all": {}},
                size=1000,  # å‡è®¾ä¸ä¼šè¶…è¿‡1000ä¸ªæ–‡æ¡£
            )

            for hit in response["hits"]["hits"]:
                doc = hit["_source"]
                doc["_id"] = hit["_id"]
                docs.append(doc)

            print(f"  âœ… æˆåŠŸå¤‡ä»½äº† {len(docs)} ä¸ªæ–‡æ¡£")
            # ä¿å­˜å¤‡ä»½åˆ°æ–‡ä»¶
            backup_file = "data/raw/es_backup.json"
            with open(backup_file, "w", encoding="utf-8") as f:
                json.dump(docs, f, ensure_ascii=False, indent=2)
            print(f"  âœ… å¤‡ä»½å·²ä¿å­˜åˆ° {backup_file}")

        except Exception as e:
            print(f"  âŒ å¤‡ä»½æ•°æ®å¤±è´¥: {e}")
            return False

        # åˆ é™¤æ—§ç´¢å¼•
        print(f"ğŸ—‘ï¸ åˆ é™¤æ—§ç´¢å¼•...")
        try:
            es.indices.delete(index=index_name)
            print(f"  âœ… ç´¢å¼• {index_name} å·²åˆ é™¤")
        except Exception as e:
            print(f"  âŒ åˆ é™¤ç´¢å¼•å¤±è´¥: {e}")
            return False

    # åˆ›å»ºæ–°ç´¢å¼•ï¼Œä½¿ç”¨æ­£ç¡®çš„æ˜ å°„
    print(f"ğŸ“ åˆ›å»ºæ–°ç´¢å¼•...")

    # ç½‘é¡µç´¢å¼•é…ç½®
    webpage_mapping = {
        "mappings": {
            "properties": {
                "url": {"type": "keyword"},
                "title": {
                    "type": "text",
                    "analyzer": "default",
                    "fields": {"keyword": {"type": "keyword"}},
                },
                "content": {"type": "text", "analyzer": "default"},
                "html": {"type": "text", "store": True},  # å­˜å‚¨HTML
                "anchor_texts": {
                    "type": "nested",  # æ”¹ä¸ºåµŒå¥—ç±»å‹
                    "properties": {
                        "text": {"type": "text", "analyzer": "default"},
                        "href": {"type": "keyword"}
                    }
                },
                "domain": {"type": "keyword"},  # ç¡®ä¿æ˜¯keywordç±»å‹
                "metadata": {
                    "properties": {
                        "crawl_time": {"type": "date"},
                        "last_modified": {"type": "date"},
                        "content_type": {"type": "keyword"},
                    }
                },
                "attachments": {
                    "type": "nested",
                    "properties": {
                        "url": {"type": "keyword"},
                        "filename": {
                            "type": "text",
                            "fields": {"keyword": {"type": "keyword"}},
                        },
                        "type": {"type": "keyword"},
                        "metadata": {
                            "properties": {
                                "title": {"type": "text", "analyzer": "default"},
                                "author": {"type": "text"},
                                "upload_date": {"type": "date"},
                                "file_size": {"type": "long"},
                            }
                        },
                    },
                },
            }
        },
        "settings": {
            "number_of_shards": 3,
            "number_of_replicas": 1,
            "analysis": {
                "analyzer": {"default": {"type": "custom", "tokenizer": "ik_smart"}}
            },
        },
    }

    try:
        es.indices.create(index=index_name, body=webpage_mapping)
        print(f"  âœ… ç´¢å¼• {index_name} å·²åˆ›å»ºï¼Œä½¿ç”¨æ­£ç¡®çš„æ˜ å°„")
    except Exception as e:
        print(f"  âŒ åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
        return False

    # å¯¼å…¥å¤‡ä»½æ•°æ®
    
    if docs:
        print(f"ğŸ“¤ å¯¼å…¥å¤‡ä»½æ•°æ®...")
        success = 0
        errors = 0

        for doc in docs:
            doc_id = doc.pop("_id", None)
            
            # å¤„ç†anchor_textså­—æ®µæ ¼å¼é—®é¢˜
            if "anchor_texts" in doc:
                # æ£€æŸ¥å½“å‰æ ¼å¼å¹¶è½¬æ¢ä¸ºæ­£ç¡®çš„åµŒå¥—æ ¼å¼
                anchor_texts = doc["anchor_texts"]
                # å¦‚æœä¸æ˜¯åˆ—è¡¨ï¼Œå…ˆè½¬ä¸ºç©ºåˆ—è¡¨
                if not isinstance(anchor_texts, list):
                    anchor_texts = []
                
                # å¦‚æœåˆ—è¡¨ä¸­çš„å…ƒç´ ä¸æ˜¯å­—å…¸æˆ–ç¼ºå°‘å¿…è¦çš„å­—æ®µï¼Œè¿›è¡Œè½¬æ¢
                normalized_anchors = []
                for anchor in anchor_texts:
                    if isinstance(anchor, dict) and "text" in anchor and "href" in anchor:
                        # å·²ç»æ˜¯æ­£ç¡®æ ¼å¼
                        normalized_anchors.append(anchor)
                    elif isinstance(anchor, str):
                        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°†å…¶è½¬ä¸ºå­—å…¸æ ¼å¼
                        normalized_anchors.append({"text": anchor, "href": ""})
                    elif isinstance(anchor, dict):
                        # å¦‚æœæ˜¯å­—å…¸ä½†ç¼ºå°‘å­—æ®µï¼Œè¡¥å……
                        text = anchor.get("text", "")
                        href = anchor.get("href", "")
                        if not text and "anchor_text" in anchor:
                            text = anchor["anchor_text"]
                        normalized_anchors.append({"text": text, "href": href})
                
                # æ›´æ–°æ–‡æ¡£ä¸­çš„anchor_textså­—æ®µ
                doc["anchor_texts"] = normalized_anchors
            
            try:
                es.index(index=index_name, id=doc_id, document=doc)
                success += 1
            except Exception as e:
                print(f"  âš ï¸ å¯¼å…¥æ–‡æ¡£ID {doc_id} å¤±è´¥: {str(e)[:100]}...")
                errors += 1

        print(f"  âœ… å¯¼å…¥å®Œæˆ: æˆåŠŸ {success} ä¸ª, å¤±è´¥ {errors} ä¸ª")

    # åˆ·æ–°ç´¢å¼•
    es.indices.refresh(index=index_name)
    print(f"âœ… ç´¢å¼•æ˜ å°„ä¿®å¤å®Œæˆ!")

    return True


def main():
    """ä¸»å‡½æ•°"""
    success = fix_es_mapping()
    return 0 if success else 1


if __name__ == "__main__":
    import sys

    sys.exit(main())
